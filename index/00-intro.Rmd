<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

```{r load_pkgs, message=FALSE, echo=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
# Load packages
library(aggiedown)
library(knitr)
```

# Introduction {-}

## Motivation

Large-scale genome sequencing is a paradigm-shifting technology for biology of the 21st century.
Since the sequencing of the human genome was declared complete in April of 2003, genomics has been a core part of the public conscience and rapidly grown as a tool for biological research.
What started as a small-scale molecular biology method for understanding individual genes grew into projects mapping out the genomes of most major model organisms in great functional detail, sequencing entire populations of microbes from the environment, mapping hundreds of thousands of individual human genomes, and producing draft sequences for hundreds of thousands of species across the entire tree of life; as of 2021, the National Center for Biotechnology Information (NCBI) stored over 36 petabytes of raw sequencing information in its Sequence Read Archive (SRA), with many times that volume not submitted.



## Background

### Streaming Algorithms

Streaming, or online, algorithms are those that process data in the *stream model*: a stream of data is fed to the algorithm, one observation at a time in the order of arrival, and each item is processed at most once.
Alternatively, we can say that a streaming algorithm makes at most one pass over the input data.
The stream may either be finite or infinite; when we conceptualize by number of passes, we generally are considering finite streams.
A closely related group is the semi-streaming algorithm, which are those that make *few* passes over the data, typically two or less.
These algorithms are of course concerned with finite streams of data.

Streaming approaches find the most use when data is too large to fit in main memory; for example, when working with massive network traffic data one may only be concerned with some properties of the underlying graph which itself is much too large for main memory, necessitating streaming approaches that drill down specifically to those properties.
In a genomics context, we might be interested in analyzing a set of sequencing reads without generating intermediate files or making multiple passes over large text files of sequences.

The data stream model was initially formalized by researchers at Digital Systems Research Center [@doi:10.1090/dimacs/050/05].
They define a *data stream* as "a sequence of data items $x_1 \ldots x_i \ldots x_n$ such that the items are read once in increasing order of the indices $i$"; this definition has persisted to today.
Their work further parameterized a computational model by the number of passes $P$, with work built on this definition sometimes referring to algorithms where $P > 1$ categorized as semi-streaming.
While there is a paucity of surveys considering streaming algorithms in *general*, several focus on more specific applications while giving some generalized overview of streaming models.
In [@doi:10.1007/978-1-4419-6045-0_13], streaming algorithms for massive graphs are explored.
[@doi:10.1145/543613.543615] explores the streaming model in contrast to traditional database management approaches, and further subdivides streaming methods: approximate query processing, sliding window approaches which consider only recent elements of a stream, sampling and "synopsis" approaches for when the rate of a stream is too high, and the tradeoffs between batching, sampling, and sketching.
*Data Streams: Algorithms and Applications* [@doi:10.1561/0400000002] is a textbook survey of broader data stream problems, of which streaming algorithms are a subset.

#### Streaming in Omics

With omics data generation continuing to accelerate, interest in streaming approaches has grown.
There are a number of advantages to a transition to streaming approaches in omics:

##### The "Data Deluge"

Even with per-base sequencing costs stabilizing, the number of sequences generated by various institutions worldwide keeps growing at a geoemetric rate.
However, the actual cost of the sequencing itself is dwarfed by the cost of the computing and downstream analysis of the data [@doi:10.1186/s13059-016-0917-0].
Furthermore, the adoption of cloud-computing has shifted the computing costs for many institutions from a fixed to a variable-cost model; this model incentivizes minimizing intermediate file storage and efficient algorithms that are more easily distributed in a principled, independent way.
Streaming approaches can reduce I/O and bandwidth mid-stream while transferring data to cloud services; they can help remove intermediate files that increase variable costs; and they can help decompose problems for distribution across disparate cloud systems.
Streaming thus helps address both data volume and shifting funding models.

##### Real-time Sequencing Strategies

While Next Generation Sequencing (NGS) technology operates in a massively parallel fashion, real-time single-molecule sequencing technologies like nanopore sequencing are slowly gaining primacy.
In this new paradigm, polynucleotide molecules are moved through a pore and observed one base at a time, meaning that the process of sequencing itself is made streaming.
These technologies allow much longer sequence lengths, and as their error rates decrease, it is expected they will supplant NGS technologies.
As real-time sequencing becomes the norm, so will streaming algorithms that can analyze sequence as soon as they are available from the machine.
Indeed, these sorts of approaches are already being developed, with algorithms for pathogen detection [@doi:10.1186/s13742-016-0137-2], barcode demultiplexing [@pubmed:28961965], and basecalling [@doi:10.1093/gigascience/giy037] already published.

##### Data Accessibility

Streaming can act as a data accessibility multiplier.
Institutions with limited resources benefit greatly from reduced storage requirements, as storage can represent a significant up-front cost.
Reduced data requirements may also translate to reduced network requirements, which helps with accessiblity in locations where network access is at a premium.
Finally, streaming approaches often go hand-in-hand with sketching approaches, further reducing computational requirements and helping to bring omics analysis to individual users.

### Sequence Assembly: A Fundamental Problem in Genomics

Assembly refers to the reconstruction of a set of hidden sequences $G_h$ from some set of fragments $R$ of length $L$ randomly sampled from $G_h$, with $G_h$ being the underlying genome or transcriptome of an organism (or organisms) being sequenced.
The introduction of shotgun sequencing in 1979 [@simpson_theory_2015; @staden_strategy_1979] created a concrete need for algorithmic solutions to the assembly problem, the first complexity analysis of a simplified sequence assembly problem having already been carried out in 1978 [@maier_complexity_1978].
The first simple genome assembler came later on in 1988, using a greedy overlap-layout-consensus (OLC) approach [@tarhio_greedy_1988].
The simplest approach, which @maier_complexity_1978 described, is casting it as the NP-Complete shortest common supersequence (SCS) problem.
This approach is inadequate for actual samples, as genomes are plagued by repetitive sequence:  tandem repeats (the same substring repeated back-to-back many times), interspersed repeats (from structures like transposable elements), and paralogs (genes which are repeated within a genome).
Overlap detection is further complicated by error intrinsic to the sequencing process.
Regardless of origin, these repeat classes all introduce a major challenge to assembly, and as pointed out in @simpson_theory_2015, break the SCS assumption of parsimony and drive the majority of work in the field.

### de Bruijn Graphs, Assembly Graphs, and their Uses

Although other approaches exist [@huang_cap3:_1999; @tarhio_greedy_1988], almost all modern assemblers rely on the assembly graph paradigm.
Broadly, a graph is built with sequence fragments as the nodes and overlaps between them as the edges, and simplified to remove transitive redundancy.
Appropriate traversals on this graph yield sequences from $G_h$ [@myers_toward_1995].
The method of construction and structure further leads to two formulations of the assembly graph: the string graph [@myers_fragment_2005] and de Bruijn graph.
While the string graph method is commonly used [@gonnella_readjoiner:_2012; @simpson_efficient_2012; @li_exploring_2012] and important, in this work I focus on the de Bruijn graph method; both methods are bounded by the same complexity constraints [@medvedev_computability_2007], though each has its own practical and theoretical
advantages.

A de Bruijn graph (dBG) $G(k, \Sigma)$ is a directed graph defined by its nodes, which are a set of fixed-length sequences of length $k$ over an alphabet $\Sigma$, and its edges, which exist when the sequences of a pair of nodes share a length $k-1$ suffix and prefix.
These length-$k$ sequences are referred to as $k$-mers, and the terms nodes and $k$-mers may be used interchangeably.
This means that $G(k, \Sigma)$ has a maximum number of nodes $\| \Sigma \|^k$ and each node has a maximum in and out degree of $\| \Sigma \|$.
If we define $pre(n_i)$ to be the length $k-1$ prefix of some node $n_i \in G$ and $suf(n_i)$ to be the length $k-1$ suffix, the out-neighbors of $n_i$ can be discovered by querying $\{ suf(n_i) + s,  \forall s \in \Sigma \}$ and the in-neighbors accordingly: that is to say, in this construction, known as the *node-centric* de Bruijn graph, the edges are implied by the set of nodes, and it is only necessary to use an efficient set data structure to implement a basic dBG.

Let $\delta_{in}(v)$ be the in-degree of a node $v$, and $\delta_{out}(v)$ be the out-degree of a node $v$.
A node $v$ where $\delta_{in}(v) > 1$ or $\delta_{out}(v) > 1$ is a *decision node*, and its associated $k$-mer a *decision $k$-mer*.

de Bruijn graphs have been used to great effect in genomics applications since they were first applied to them [@myers_toward_1995].
The principle advantage of the dBG is its simplification of the process of overlap detection between sequences: $k$-mers can stored as hashed integers for faster querying, and length $k-1$ overlaps can be discovered by using the same method as edge detection.
Hence, when a collection of sequences sampled from an underlying larger set of sequences is broken down into its constituent $k$-mers as a dBG, all $k$-mer overlaps are known implicitly. This property, and the methods extending it, can be used for genome assembly, sequence classification and comparison, genomic variant discovery, and full graph-based alignment.

### Compact de Bruijn Graphs

While the standard de Bruijn graph is a useful abstraction, in practice, most implementations only use it as an intermediate representation.
The reasons are twofold: firstly, because genomic dBGs tend to consist of many linear paths of $k$-mers with in- and out-degree of $1$, such graphs contain large quantities of redundant $k$-mers; secondly, because $k$-mers are exact substrings over a sliding window on the input sequences, each error or point mutation in an input sequence can introduce up to $k$ new nodes in the graph.
As a result, even low error and polymorphism rates in moderate sized data produce graphs with hundreds of millions or billions of nodes.

These problems are addressed by the compact(ed) de Bruijn Graph (cDBG).
In the cDBG, paths of the form $v_0 \ldots v_i \ldots v_n$, where  $\delta_{in}(v_{1..n-1})$ and $\delta_{out}(v_{1...n-1})$ are exactly $1$, are merged into single nodes; that is, linear paths are reduced to single nodes defined by their sequence, and the ends of these sequences may be either tips (that is, they have degree zero) or decision $k$-mers.
This representation preserves the implicit edge definition, while avoiding the high overhead of storing redundant $k$-mers.

The compacted nodes, and their associated sequences, are referred to as *unitigs*.
A unitig that can not be extended in either direction is a *maximal unitig*: this is, $\delta_{in}(v_0) = 0$ or $v_0$ is a decision node, and $\delta_{out}(v_n) = 0$ or $v_n$ is a decision node.
The term *unitig* was first introduced in [@movahedi2012novo], where the process of forming unitigs was referred to as "condensation."
*Compaction* and *compact* or *compacted* de Bruin graph have since become the accepted terms.
The most basic process of compaction was earlier described and implemented for the Velvet assembler [@pmc:PMC2336801], the first mainstream de Bruijn graph assembler; here it was introduced as a lossless graph simplification method.

There are a variety of data models for representing the cDBG.
The simplest is a flat collection of unitigs; edges can be inferred by storing only the unitig ends, which correspond to the decision $k$-mers and tips, and querying these ends for neighbors.
How this subset of $k$-mers is stored can vary by implementation, with simpler models using a hash table directly [@doi:10.1101/gr.074492.107; @doi:10.1101/gr.089532.108], and later work transitioning to succinct data structures like the FM-Index [@doi:10.1109/SFCS.2000.892127].
Regardless, a key observation is that the compact de Bruijn graph is defined by its set of decision $k$-mers, which, along with the formalization of the node-centric de Bruijn graph [@chikhi2016compacting] (where edges are stored implicitly, as defined previously), brings about the concept of Navigational Data Structures for assembly [@DOI:10.1089/cmb.2014.0160].
[@DOI:10.1089/cmb.2014.0160] provides a good background survey of the methods used for navigational data structures up until 2015; since then, considerable work has gone into augmenting the graph with additional information, such as the colored compact de Bruijn graph, which is beyond the scope of this work.

Notably, the succinct data structures, while offering query and storage efficiency, are not straightforward to use in a dynamic fashion.
Simpler membership structures, like the hash tables used in [@doi:10.1101/gr.089532.108; @pmc:PMC2336801] or Bloom filters [@doi:10.1073/pnas.1121464109], allow dynamic insertion of new $k$-mers, and hence edges, using the node-centric model.
Dynamic data structures that allow efficient insertion and query are key to enabling streaming approaches.

### Conclusion

The rapid and growing generation of sequence data, the advent of real-time sequencing technologies, and the need for accessible genomics analysis for a wide range of consumers can be partially addressed by the adoption of streaming approaches.
While some methods have already been adapted for streaming, de Bruijn graph compaction, a key component of assembly pipelines, has not; further, sublinear methods remain uncommon.
Here, we will first focus on adapting compaction for streaming, and show how this new paradigm also enables novel analysis methods for assembly graph structure.
Then, building on that work, we will explore preliminary sub-linear approaches for filtering sequences prior to compaction and further downstream analysis.
Our goal is to provide a framework for more streaming methods to build on in the future: we hope that, as technology and algorithms improve, we will eventually see the evolution of end-to-end streaming pipelines, from sequencing instrument to biological inquiry.
