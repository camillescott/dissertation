<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

```{r load_pkgs, message=FALSE, echo=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
# Load packages
library(aggiedown)
library(knitr)
```

# Introduction {-}

## Motivation

* Scaling sequence analysis
* Improving accessibility
* Overly-deep pipelines / intermediate data
* Sea-change toward streaming methods due to nanopore

Large-scale genome sequencing is a paradigm-shifting technology for biology of the 21st century.
Since the sequencing of the human genome was declared complete in April of 2003, genomics has been a core part of the public conscience and rapidly grown as a tool for biological research.
What started as a small-scale molecular biology method for understanding individual genes grew into projects mapping out the genomes of all major model organisms in great functional detail, sequencing entire populations of microbes from the environment, mapping hundreds of thousands of individual human genomes, and producing draft sequences for hundreds of thousands of species across the entire tree of life; as of 2021, the National Center for Biotechnology Information (NCBI) stored over 36 petabytes of raw sequencing information in its Sequence Read Archive (SRA), with many times that volume not submitted.



## Background

* Sequencing technology
    * Second-gen HTS: illumina
    * Long read tech: pacbio
    * Real-time sequencing: nanopore
* Streaming Algorithms
    * Advantages
        * Implied dynamic data structures
        * Disk space efficiency
        * Removal of intermediate output
        * Infinite-size data
        * Real-time analysis

* de Bruijn Graphs
    * Representations
        * Offline
    * Tagging / partitioning
        * Why
            * seeding for traversal
            * alignment-free comparison
            * locking / parallel construction
            * sparse-graph construction
        * Greedy k-mer subset (khmer, SparseAssembler)
        * (W,K) Minimizers
        * r-dom sets
        * Universal k-mer hitting sets
    * Implementations: offline
        * So many k-mer counters
        * GATB
        * So many assemblers: AbySS, velvet, IDBA, megahit, ALLPATHS
    * Implementations: streaming
        * khmer/oxli
* compact de Bruijn graphs
    * Implementations: offline
        * BCALM
        * TwoPaCo
        * As components of assemblers: Abyss, etc
    * Implementations: semi-streaming
        * Faucet
        * LightAssembler
* saturation
    * Definitions of saturation
        * Accumulation of information vs error
        * by minimizing fragmentation
        * by coverage profile
        * by completeness: gene content, etc (BUSCO)
    * By context
        * Genomic vs txomic vs mtomic
        * downstream goals: assembly, classifcation, abundance
* Sketching



### Streaming Algorithms

Streaming, or online, algorithms are those that process data in the *stream model*: a stream of data is fed to the algorithm, one observation at a time in the order of arrival, and each item is processed at most once.
Alternatively, we can say that a streaming algorithm makes at most one pass over the input data.
The stream may either be finite or infinite; when we conceptualize by number of passes, we generally are considering finite streams.
A closely related group is the semi-streaming algorithm, which are those that make *few* passes over the data, typically two or less.
These algorithms are of course concerned with finite streams of data.

Streaming approaches find the most use when data is too large to fit in main memory; for example, when working with massive network traffic data one may only be concerned with some properties of the underlying graph and which itself is much too large for main memory, necessitating streaming approaches that drill down specifically to those properties.
In a genomics context, we might be interested in analyzing a set of sequencing reads without generating intermediate files or making multiple passes over large text files of sequences.

#### Streaming in Omics

With omics data generation continuing to accelerate, interest in streaming approaches has grown.
There are a number of advantages to a transition to streaming approaches in omics:

##### The "Data Deluge"

Even with per-base sequencing costs stabilizing, the number of sequences generated by various institutions worldwide keeps growing at a geoemetric rate.
However, the actual cost of the sequencing itself is dwarfed by the cost of the computing and downstream analysis of the data [@doi:10.1186/s13059-016-0917-0].
Furthermore, the adoption of cloud-computing has shifted the computing costs for many institutions from a fixed to a variable-cost model; this model incentivizes minimizing intermediate file storage and efficient algorithms that are more easily distributed in a principled, independent way.
Streaming approaches can reduce I/O and bandwidth mid-stream while transferring data to cloud services; they can help remove intermediate files that increase variable costs; and they can help decompose problems for distribution across disparate cloud systems.
Streaming thus helps address both data volume and shifting funding models.

##### Real-time Sequencing Strategies

While Next Generation Sequencing (NGS) technology operates in a massively parallel fashion, real-time single-molecule sequencing technologies like nanopore sequencing are slowly gaining primacy.
In this new paradigm, polynucleotide molecules are moved through a pore and observed one base at a time, meaning that the process of sequencing itself is made streaming.
These technologies allow much longer sequence lengths, and as their error rates decrease, it is expected they will supplant NGS technologies.
As real-time sequencing becomes the norm, so will streaming algorithms that can analyze sequence as soon as they are available from the machine.
Indeed, these sorts of approaches are already being developed, with algorithms for pathogen detection [@doi:10.1186/s13742-016-0137-2], barcode demultiplexing [@pubmed:28961965], and basecalling [@doi:10.1093/gigascience/giy037] already published.

##### Data Accessibility

Streaming can act as a data accessibility multiplier.
Institutions with limited resources benefit greatly from reduced storage requirements, as storage can represent a significant up-front cost.
Reduced data requirements may also translate to reduced network requirements, which helps with accessiblity in locations where network access is at a premium.
Finally, streaming approaches often go hand-in-hand with sketching approaches, further reducing computational requirements and helping to bring omics analysis to individual users.

### Sequence Assembly: A Fundamental Problem in Genomics

Assembly refers to the reconstruction of a set of hidden sequences $G_h$ from some set of fragments $R$ of length $L$ randomly sampled from $G_h$, with $G_h$ being the underlying genome or transcriptome of an organism (or organisms) being sequenced.
The introduction of shotgun sequencing in 1979 [@simpson_theory_2015; @staden_strategy_1979] created a concrete need for algorithmic solutions to the assembly problem, the first complexity analysis of a simplified sequence assembly problem having already been carried out in 1978 [@maier_complexity_1978].
The first simple genome assembler came later on in 1988, using a greedy overlap-layout-consensus (OLC) approach [@tarhio_greedy_1988].
The simplest approach, which @maier_complexity_1978 described, is casting it as the NP-Complete shortest common supersequence (SCS) problem.
This approach is inadequate for actual samples, as genomes are plagued by repetitive sequence:  tandem repeats (the same substring repeated back-to-back many times), interspersed repeats (from structures like transposable elements), and paralogs (genes which are repeated within a genome).
Overlap detection is further complicated by error intrinsic to the sequencing process.
Regardless of origin, these repeat classes all introduce a major challenge to assembly, and as pointed out in @simpson_theory_2015, break the SCS assumption of parsimony and drive the majority of work in the field.

### de Bruijn Graphs, Assembly Graphs, and their Uses

Although other approaches exist [@huang_cap3:_1999; @tarhio_greedy_1988], almost all modern assemblers rely on the assembly graph paradigm.
Broadly, a graph is built with sequence fragments as the nodes and overlaps between them as the edges, and simplified to remove transitive redundancy.
Appropriate traversals on this graph yield sequences from $G_h$ [@myers_toward_1995].
The method of construction and structure further leads to two formulations of the assembly graph: the string graph [@myers_fragment_2005] and de Bruijn graph.
While the string graph method is commonly used [@gonnella_readjoiner:_2012; @simpson_efficient_2012; @li_exploring_2012] and important, in this work I focus on the de Bruijn graph method; both methods are bounded by the same complexity constraints [@medvedev_computability_2007], though each has its own practical and theoretical
advantages.

A de Bruijn graph (dBG) $G(k, \Sigma)$ is a directed graph defined by its nodes, which are a set of fixed-length sequences of length $k$ over an alphabet $\Sigma$, and its edges, which exist when the sequences of a pair of nodes share a length $k-1$ suffix and prefix.
These length-$k$ sequences are referred to as $k$-mers, and the terms nodes and $k$-mers may be used interchangeably.
This means that $G(k, \Sigma)$ has a maximum number of nodes $\| \Sigma \|^k$ and each node has a maximum in and out degree of $\| \Sigma \|$.
If we define $pre(n_i)$ to be the length $k-1$ prefix of some node $n_i \in G$ and $suf(n_i)$ to be the length $k-1$ suffix, the out-neighbors of $n_i$ can be discovered by querying $\{ suf(n_i) + s,  \forall s \in \Sigma \}$ and the in-neighbors accordingly: that is to say, in this construction, known as the *node-centric* de Bruijn graph, the edges are implied by the set of nodes, and it is only necessary to use an efficient set data structure to implement a basic dBG.

Let $\delta_in(v)$ be the in-degree of a node $v$, and $delta_out(v)$ be the out-degree of a node $v$.
The nodes in a dBG with in on out-degree greater than $1$ are *decision nodes*, their associated $k$-mers *decision $k$-mers*.

de Bruijn graphs have been used to great effect in genomics applications since they were first applied to them [@myers_toward_1995].
The principle advantage of the dBG is its simplification of the process of overlap detection between sequences: $k$-mers can stored as hashed integers for faster querying, and length $k-1$ overlaps can be discovered by using the same method as edge detection.
Hence, when a collection of sequences sampled from an underlying larger set of sequences is broken down into its constituent $k$-mers as a dBG, all $k$-mer overlaps are known implicitly. This property, and the methods extending it, can be used for genome assembly, sequence classification and comparison, genomic variant discovery, and full graph-based alignment.

### Compact de Bruijn Graphs

While the standard de Bruijn graph is a useful abstraction, in practice, most implementations only use it as an intermediate representation.
The reasons are twofold: firstly, because genomic dBGs tend to consist of many linear paths of $k$-mers with in- and out-degree of $1$, such graphs contain large quantities of redundant $k$-mers; secondly, because $k$-mers are exact substrings over a sliding window on the input sequences, each error or point mutation in introduces $k$ new nodes in the graph.
As a result, even small error and polymorphism rates in moderate sized data produce graphs with hundreds of millions or billions of nodes.

These problems are addresses by the Compact de Bruijn Graph (cDBG).
In the cDBG, paths of the form $v_0 \rightarrow ... v_n$ where $delta_out(v_0) = 1$, $delta_in(v_{1..n-1})$ and $delta_out(v_{1...n-1})$ are $1$, and $delta_in(v_n) = 1$ are compacted into single nodes; that is, linear paths are reduced to single nodes defined by their sequence, and the ends of these sequences may be either tips (that is, they have degree zero) or decision $k$-mers.
This representation preserves the implicit edge definition, while avoiding the high overhead of storing redundant $k$-mers.

The compacted nodes, and their associated sequences, are referred to as *unitigs*.
A unitig that can not be extended in either direction is a *maximal unitig*.

#### cDBG Representations

There are a variety of data models for representing the cDBG.
The simplest is a flat collection of unitigs; edges can be inferred by storing only the unitig ends, which correspond to the decision $k$-mers and tips, and querying these ends for neighbors.


#### de Bruijn Graph Tagging and Partitioning

