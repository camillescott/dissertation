# Streaming Signature Saturation {#streaming-sketching}

<!-- This adds a different "short title" -->
\chaptermark {Signature Saturation}

\begin{sizepar}{16}{24}
\begin{description}

\item[Chapter Authors]
Camille Scott and Luiz Irber

\end{description}
\end{sizepar}



## Introduction

In the previous chapter, we showed that streaming de Bruijn graph compaction opens new avenues for exploring assembly graph architecture. 
In particular, we can directly explore architecture as a function of coverage by tracking changes to the graph over time.
Many metrics tend toward stability as more sequence is consumed, suggesting an avenue for sub-linear approaches.
Sub-linear algorithms are those which produce a result without examining an entire data stream.
Such algorithms, in general, are concerned with *property testing*: producing a yes or no answer as to whether input data satisfies some constraint [@doi:10.1137/100791075].
For most such problems, an exact solution is clearly impossible in sub-linear time; however, with fields where data generation outpaces data analysis, quickly producing approximate solutions can be preferable to slowly producing exact solutions.
The "data deluge" of genomics has made the field a prime candidate for approximate and probabilistic approaches, with many such methods developed and improved upon in recent years [@doi:10.1146/annurev-biodatasci-072018-021156; @doi:10.1186/s13059-019-1809-x].
Sketching approaches, especially those built on MinHash, have received considerable attention for the ability to efficiently classify and cluster many large samples in low memory.

In this work, we perform a preliminary exploration of sketching for intra-sample comparison.
We focus on the stability of sketches during the data stream, where refer to high stability as saturation.
The authors of Mash point out the potential of comparing a stream to a known database and terminating after meeting a classification threshold [@doi:10.1186/s13059-016-0997-x].
Here, we explore the potential of comparing a series of sketches of a stream to themselves during construction.
Our expectation is to observe saturation behavior in these streams, opening the door for a sub-linear method without the need for a reference database.
We implement a set of tools for streaming sketch analysis in the `goetia` software [@url:https://github.com/camillescott/goetia] and demonstrate its use with MinHash and FracMinHash sketches, and with a novel type of sketch tailored to saturation detection we refer to as the Draff sketch.

## Methods {#chap2-methods}

### The Draff Sketch

The `draff` sketch is, in brief, a fixed-length vector where each position corresponds to the number of unique $W$-mers from a sequence sample that map to a $k$-mer in a Universal $k$-mer hitting set $UHS(W,k)$.
The universal $k$-mer hitting set (UHS) is a set of $k$-mers from an alphabet $\Sigma$ where any possible sequence of length $W$ over $\Sigma$ must contain a $k$-mer from the set [@doi:10.1371/journal.pcbi.1005777].
Universal $k$-mer hitting sets are an improvement over classic minimizer schemes, in that they are pre-computed and hence fixed-length any set of input sequences, and that they are smaller than minimizer schemes over the same $W$ and $k$.
The `draff` sketch takes advantage of this fixed-length property to produce compressed representations that are comparable between samples.
Each $k$-mer in the set is assigned a position in the sketch vector and a storage backend; the backend can be any $k$-kmer data structure that can count unique members, with our reference implementation supporting exact sets, Bloom filters, and HyperLogLog [@doi:10.46298/dmtcs.3545] counters.
For each sequence $s_t$ in the sequence stream $S$, each $W$-mer from $s_t$ is assigned to a $k$-mer from the hitting set, with the $k$-mer with the smallest hash value being chosen if there is mere than one.
The $W$-mer is then added to the counting backend associated with the assigned $k$-mer.
The sketch vector is computed by querying the number of unique $W$-mers associated with each $k$-mer in the UHS.

The size of the sketch depends on the parameters $W$ and $k$.
Sketches built from a larger underlying UHS (which will be those with larger values of $k$) will have a higher resolution; that is, were theoretical hitting sets be developed with values up to $W = k$, the resulting sketch would converge to a bit vector with length equal to $\|\Sigma\|^W$, or in other words, a position for every $W$-mer.
Practical values of $k$ for which we have precomputed hitting sets are $k = \{7,8,9,10\}$ with $W$ values up to 200.
For example, the UHS with $W=31$ and $k=9$ has $41,346$ hitting $k$-mers and hence the draff sketch has this same length.

$W$-mers and $k$-mers are hashed simultaneously using a rolling hash scheme [@lemire2010recursive].
To efficiently track the minimum $k$-mer for determining a hitting $k$-mer, we use an ascending minima scheme [@url:https://people.cs.uct.ac.za/~ksmith/articles/sliding_window_minimum.html].
When using full set backends such as the Bloom filter or exact sets, the data structure is inherently a partitioned de Bruijn Graph; this partitioning scheme also allows for straightforward multithreading support.

### `sourmash` MinHash Sketches

For comparison, we make use of the MinHash sketch for genomics [@doi:10.1186/s13059-016-0997-x] as implemented in the `sourmash` package [@doi:10.21105/joss.00027], as well as the FracMinHash sketch further built on in `sourmash` [@doi:10.1101/2022.01.11.475838].
MinHash sketching is related to shingling and minimizer schemes.
For some sketch size $N$, length $k$, and random hash function $f$, the $k$-mers from sequences $s_t \in S$ of a sequence stream are hashed with $f$ and the $N$ hashes with lowest value kept; hence, these methods are sometimes referred to as bottom sketches.
The resulting set of size $N$ is the MinHash sketch, and sketches can be compared to each other with the Jaccard similarity methods.
The closely related FracMinHash extends MinHash to non-fixed sizes.
Instead of keeping the bottom $N$ hashes, a cutoff hash value proportional to the size of the hash space is chosen, and all hash values less than this cutoff are kept.
Thus, the FracMinHash grows in proportion to the input data, and represents a random sample of $k$-mers with probability of the cutoff value divided by the maximum hash value.
While this means FracMinHashes have unbounded growth, it also allows implementation of additional operations such as containment queries.

### Timing and Distance Metrics {#chap2-methods-timing}

As our methods rely on measuring changes in our underlying sketches throughout a stream, robustly determining when and how to make these measurements is fundamental.
The fundamental unit of time in our framework is the observed $k$-mer: time $t$ is incremented for each $k$-mer in the stream.
As such, a single sequence fragment $s$ covers an $\texttt{len}(s) - K + 1$-length time window in the stream.
Using $k$-mers instead of sequences makes time deltas comparable between samples with different sequence lengths and input sequences over varying size, for example, those that have been subjected to quality or abundance trimming.
Timing of distance measurements and windowed operations is performed at a parameterized frequency interval $I$.
The time $t$ is compared to the previous recorded time $t_p$; when $t - t_p \geq I$, the queued measurement operations are performed and $t_p$ is set to $t$.
The implementation of this system is done at the parsing level.
When a new sequence $s_i \in S$ is parsed, and its $k$-mers consumed by the underlying analyzer (sketch, dBG, filter, compactor, or otherwise), $t$ is incremented by $\texttt{len}(s) - K + 1$ and the prior comparison made.
If $I$ is exceeded, parsing yields to metrics operations.
In this system, the actual length of measurement deltas will differ from $I$ by some positive value $\epsilon$; however,
$I \gg L$, generally exceeding $500,000$, so $\varepsilon$ is trivial.
Excepting a small $\varepsilon$ allows sequence consumers to benefit from the entire connected path of $k$-mers from a fragment $s$ when necessary.

Let the time points corresponding to these intervals be referred to as $i_0 \ldots i_j \ldots i_N$, where $N$ is the $k$-mer time length of the stream, and the sketch associated with these time points as $A_0 \ldots A_j \ldots A_N$.
During streaming sketching, the distance functions $\delta ( A_j, A_{j+1} )$ are called between sketches at successive intervals, and the result $d_j$ added to the distance list $\Delta = \{ d_0 \ldots d_j \ldots d_{N-1} \}$.
When our sketch is a `sourmash` Min-hash sketch, $\delta$ is $1 - Jaccard$, where $Jaccard$ is the Jaccard similarity.
For the `draff` sketch, $\delta$ can be any standard distance function: we commonly use the Euclidean, cosine, correlation, and Canberra [@doi:10.1093/comjnl/9.1.60] distances.

## Results

### Rolling Signature Distances Display Saturation Behavior

Fig. \@ref(fig:fig-chap2-sketch-saturation) demonstrates saturation behavior of FracMinHash sketches on a transcriptomic sample taken from the MMETSP project [@doi:10.1371/journal.pbio.1001889; @doi:10.1093/gigascience/giy158].
The y-axis shows the Jaccard distances $d_j$ between the sketches at measurement intervals $i_j$ and $i_{j-1}$.
Each line represents a different measurement interval length $I$, that is, successively lower measurement frequencies.
Clearly, successive comparisons require normalization by sampling interval; expected distance between successive sketches is necessarily conditioned on the size of the underlying set of sequences (transcripts or chromosomes) and the error rate of the sequence instrument.
For all sampling intervals, the successive distances (similarities) converge toward $1$: the further into the stream, the more new $k$-mers within a sampling window are dominated by sequencing error.
The middle plot compares each the sketch at each time point $i_j$ to the final sketch at point $t_N$.
Here, we hope to see the distance curve climb steeply, then become more linear upon saturation; that is, its derivative should become constant.
In the rightmost plot, to help remove the effects of erroneous $k$-mers, we instead compare to a sketch built from an assembled and processed transcriptome from the sample.
Here, the curve much more dramatically flattens, but does not approach $1$.
Visually, we can see that the transition occurs at a similar point in the stream as in the middle plot, which lends support to our saturation hypothesis.

```{r fig-chap2-sketch-saturation, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-sketch-saturation)", fig.align="center", out.width="100%"}
include_graphics("figure/chap2/sketching-saturation-curve.png")
```

(ref:fig-chap2-sketch-saturation) **Sketching distance curves showing saturation of a transcriptomic sample.**
Sourmash FracMinHash sketch at scaled=1000, $K$=31, computed on a 21.5 million read transcriptomic sample. *Left*: Jaccard similarity between successive sketches, computed at varying sampling intervals. *Middle*: Jaccard distances between sketches at given time intervals and the final sketch. *Right*: Jaccard distances between sketches at given time intervals and the downstream sequences as assembled with Trinity in [@doi:10.1093/gigascience/giy158].

```{r fig-chap2-sourmash-diginorm, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-sourmash-diginorm)", fig.align="center", out.height="6in"}
include_graphics("figure/chap2/sourmash-diginorm-rolling-wgs-txome.png")
```

(ref:fig-chap2-sourmash-diginorm) **Streaming Jaccard Similarity of digitally-normalized sourmash-MinHash signatures.**
MinHash sketches computed with `sourmash` using $K=31$ and $N=25000$. Distances are $1.0$-Jaccard similarity between successive sketches, computed at an interval of 2000000 $k$-mers. Digital normalization is used as a pre-filter to the sketch stream, with each curve representing a difference coverage cutoff $C$. Higher coverage thresholds admit more sequences, causing the lower-threshold streams to saturate more slowly at a given *relative* position in the stream.

```{r fig-chap2-sourmash-diginorm-single, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-sourmash-diginorm-single)", fig.align="center", out.width="100%"}
include_graphics("figure/chap2/sourmash-diginorm-rolling-wgs-txome-single.png")
```
(ref:fig-chap2-sourmash-diginorm-single) **Streaming Jaccard Similarity of sourmash-MinHash signatures.**
Distance curves of the same samples as in Fig. \@ref(fig:fig-chap2-sourmash-diginorm) at digitally-normalized maximum coverage of $20$.

Fig. \@ref(fig:fig-chap2-sketch-saturation) showed FracMinHash sketches, which, while allowing greater sensitivity, have unbounded size.
For streams of sequence fragments, as opposed to assembled genomes, this property becomes problematic, with sketches growing to unwieldy sizes due to low-coverage and erroneous $k$-mers.
Figs. \@ref(fig:fig-chap2-sourmash-diginorm) and \@ref(fig:fig-chap2-sourmash-diginorm-single) show distance curves for regular MinHash sketches with size $N=25000$.
As we can see, the standard MinHash sketch also shows saturation behavior.
The lower-coverage RNA-Seq samples, for example the top-right sample with approximately $1.4$m sequences, show saturation at relative positions much further into the stream, and also show a wider spread between the different coverage levels of digital normalization.
For the $70m$ sequence whole-genome sample of *C. elegans*, in the bottom left, change in distance stabilizes much sooner.
We posit two reasons for this: firstly, the (average) coverage of this sample is much higher than the relatively smaller RNA-seq sample; and secondly, the genomic sample has more unique $k$-mers than the RNA-seq sample, with $k$-mer cardinality many orders of magnitude higher than the size of the MinHash sketch.
The latter point is of great importance, as clearly, very large MinHash sketches will be necessary to detect significant differences in the sketch throughout the stream.

### The Draff Sketch Shows Improved Saturation Behavior over MinHash

```{r fig-chap2-draff-basic, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-draff-basic)", fig.align="center", out.width="100%"}
include_graphics("figure/chap2/draff-diginorm-rolling-wgs-txome-single.png")
```

(ref:fig-chap2-draff-basic) **Streaming cosine distance of `draff` sketches.**
Draff sketches computed with $W=31$, $K=9$. On the y-axis, cosine distance calculated between successive sketches; on the x-axis, normalized time in number of $k$-mers.
On the left are the exact cosine distances; on the right, we show them on a $log$ scale.
As in \@ref(fig:fig-chap2-sourmash-diginorm-single), these are digitally normalized to a maximum coverage of 20.

draff sketches show similar saturation behavior when successive distance measurements are made.
Fig. \@ref(fig:fig-chap2-draff-basic) shows this on a selection of WGS and RNA-seq samples.
The left panel shows the cosine distance between sketches rapidly flattening early in the stream, as in the MinHash examples.
However, distances with the draff sketches can be measured at many more significant digits than the MinHash sketches, providing more sensitivity; to help highlight behavior later in the stream, on the right, we show these distances on a $log$ scale.
We can see that distances continue to reduce at a small, but steady, rate, with some exceptions.
Periodicity suggests some of the samples may have technical artifacts, with the $log$ scale amplifying their effects.

```{r fig-chap2-sourmash-draff-comp, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-sourmash-draff-comp)", fig.align="center", out.width="90%", out.height="90%"}
include_graphics("figure/chap2/sourmash-draff-ref-comparison.png")
```

(ref:fig-chap2-sourmash-draff-comp) **MinHash and draff streaming distances as compared to representative sketches.**
draff sketches with $W=31$, $K=9$ and MinHash sketches with $K=31$, $N=25000$ are compared.
The distances are between the sketch at a given time point and the final sketch in the stream, as in the middle plot of Fig. \@ref(fig:fig-chap2-sketch-saturation).

Fig. \@ref(fig:fig-chap2-sourmash-draff-comp) shows distances between the sketch at a given point and the final sketch of the stream for MinHash and draff.
Here, the draff sketch shows its greater utility for the saturation problem.
For a sketch to best capture stability *de novo*, we expect distance from the sketch of the final stream to have an inflection point that occurs when the underlying sampled sequence has been observed and new fragments transition to mostly introducing error.
The signal for the MinHash sketches is dominated by error, and they fail to capture this effect.
We posit that MinHash sketches of sufficient size should act similarly to the draff sketch; however, MinHash sketches are bound by worst-case $\mathcal{O}(log(n))$ insertion time, which imposes scaling challenges on MinHash, whereas insertion into the draff sketch is always $\mathcal{O}(1)$ (for the implemented exact and AMQ backends).

### The Draff Sketch Enables Novel Dimensional Analyses of Assembly Graphs

Because the Draff sketch is, intuitively, a compressed "snapshot" of $k$-mer space, in the form of vector of feature counts as opposed to the set of specific $k$-mer hashes in MinHash sketches, we are able use some more traditional numeric approaches with them.
In Fig. \@ref(fig:fig-chap2-draff-pca), we view the stream of sketches as a matrix with each row a draff sketch corresponding to measurement times $i_0 \ldots i_N$.
We then perform a Principle Component Analysis (PCA) along a length-10 sliding window of the rows, and extract the explained variance of the top two components for each window.
In this way, we reduce the change in the compressed "$k$-mer space" over time to a 2-dimensional representation.
This offers a different view on the saturation behavior.
Rather than distances between successive sketches decreasing and then stabilizing, we instead see the variance of a slice of sketches decreasing; moreover, a reduction in explanatory power of the top 2 components can be interpreted as the locations of new unique $k$-mers becoming more diffuse as we progress through the stream.
This provides further support for our saturation model where changes in the stream post-saturation are dominated by erroneous $k$-mers.

```{r fig-chap2-draff-pca, results="asis", echo=FALSE, fig.cap="(ref:fig-chap2-draff-pca)", fig.align="center", out.width="100%"}
include_graphics("figure/chap2/draff-windowed-pca.png")
```

(ref:fig-chap2-draff-pca) **Top-two principle components of sliding window over streaming draff sketches.**
Rather than computing distance metrics between successive pairs of sketches, Principle Component Analysis was run over a sliding window of 10 sketches at a time and explained variance of the top two components extracted. Each point is a PCA window proceeding through the sample.

## Discussion

We introduce a new sketch tailored for streaming saturation detection, which we call draff, leveraging universal hitting sets, and explore sketch saturation on it and MinHash sketches.
Both methods show stability of distances when successively compared in a stream, suggesting a method for calling saturation of a sample in a streaming context *de novo*.
The draff sketch has especially desirable properties for the task: a high level of granularity in the face of large numbers of unique $k$-mers; constant-time insertion even for large sketches; and canonical stability in self-similarity.

There are many avenues to explore here in the future.
There is the need to formally describe the relationship between sketch size, sequence error rate, and sample size in the context of self-similarity.
There is necessarily a multiple-testing problem to correct for which has not been studied here, as well as more specific limitations on comparing highly-similar MinHash sketches: for MinHashes of size $N$, the shortest distance we can compute between them is $1 - (N -1)/(N + 1)$, and whether this distance has any significance is dependent on the size of the underlying genome.
draff sketches avoid this problem by accounting for all unique $k$-mers, but better determining these limitations for this usage is of interest.
For draff, comparing the effectiveness of different distance metrics would be highly valuable, as here we only explore the cosine distance.

The previous issues directly relate to developing functions for calling saturation.
Potential functions include: a specific distance cutoff, under which we immediately terminate; a distance cutoff where we terminate after the mean distance of a sliding window meets the threshold; measuring the variance of distances over a sliding window, wherein we try to more directly measure the stability of the sketches; or other statistical moments on the stream of distances.
We briefly explored the behavior of the draff sketch in $W$-mer space via Principal Component Analysis, which opens up another potential set of methods for calling saturation.

Finally, there is the question of integrating this streaming method with the streaming compaction described in Chapter \@ref(streaming-cdbg).
Streaming saturation detection is a natural pre-filter for streaming compaction, providing a potential sub-linear solution for construction of an inexact compact de Bruijn graph.
The downstream effects on the resulting cDBG are of particular interest, as well as the results of including the method along with sequence error correction, solidity filtering, and digital normalization in pure-streaming pipelines.
Future work would focus on these pipelines holistically.
